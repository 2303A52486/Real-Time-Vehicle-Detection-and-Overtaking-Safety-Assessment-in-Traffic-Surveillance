# -*- coding: utf-8 -*-
"""RetinaNet_pipleline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F1SCEeCgsMClOfH2K2-2n677jtIM5Kb3

## Phase-1
"""

!pip install torchvision --quiet

import cv2
import torch
import torchvision.transforms as T
from torchvision.models.detection import retinanet_resnet50_fpn
import numpy as np
from collections import deque
from google.colab import files
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)

# Define vehicle classes (COCO IDs for car, motorcycle, bus, truck)
VEHICLE_CLASSES = {3: "car", 4: "motorcycle", 6: "bus", 8: "truck"}

# Define colors for vehicle classes
CLASS_COLORS = {
    3: (0, 255, 255),       # Yellow
    8: (255, 0, 0),     # Blue
    6: (0, 0, 255),       # Red
    4: (255, 255, 0),  # Cyan
    "default":(0, 255, 0)  # Green
}

# Camera calibration parameters (adjust as needed)
focal_length = 1200   # in pixels
sensor_width = 6.17   # in mm
# These can be modified/derived from your video input
# For example, if your video frame width is 640 pixels:
video_frame_width = 640

# Reuse your distance, angle, and accuracy functions from your YOLOv8 code:
def calculate_distance(bbox, label):
    # For this example, we use fixed widths for each vehicle type
    width_map = {
        "car": 1.8,
        "motorcycle": 0.8,
        "bus": 2.6,
        "truck": 2.5
    }
    real_width = width_map.get(label, 1.8)
    pixel_width = bbox[2] - bbox[0]
    return round((real_width * focal_length) / pixel_width, 2)

def calculate_overtaking_angle(bbox):
    obj_center = (bbox[0] + bbox[2]) / 2
    frame_center = video_frame_width / 2
    mm_per_pixel = sensor_width / video_frame_width
    offset_mm = (obj_center - frame_center) * mm_per_pixel
    return round(np.degrees(np.arctan(offset_mm / focal_length)), 2)

def compute_angle_accuracy(predicted_angle, true_angle=0):
    error = abs(predicted_angle - true_angle)
    accuracy = max(0, 100 - (error / 90) * 100)
    return round(accuracy, 2)

# Load RetinaNet
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = retinanet_resnet50_fpn(pretrained=True)
model.to(device)
model.eval()

# Preprocessing transformation
transform = T.Compose([T.ToTensor()])

# Draw boxes function (using our helper functions)
def draw_boxes(frame, boxes, labels, scores):
    for box, label, score in zip(boxes, labels, scores):
        if label.item() in VEHICLE_CLASSES and score.item() > 0.5:
            x1, y1, x2, y2 = map(int, box)
            cls_id = label.item()
            color = CLASS_COLORS.get(cls_id, CLASS_COLORS['default'])
            class_name = VEHICLE_CLASSES[cls_id]
            distance = calculate_distance((x1, y1, x2, y2), class_name)
            angle = calculate_overtaking_angle((x1, y1, x2, y2))
            angle_accuracy = compute_angle_accuracy(angle)
            label_text = f"{class_name}, D:{distance:.2f}m, A:{angle:.2f}Â° | Acc:{angle_accuracy:.2f}%"
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            cv2.putText(frame, label_text, (x1, max(y1 - 10, 20)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    return frame

# Process video with batch processing
def process_video(input_path, output_path="output_video.mp4", batch_size=8):
    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        logging.error(f"Error opening video: {input_path}")
        raise FileNotFoundError(f"Error opening video: {input_path}")

    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    global video_frame_width
    video_frame_width = frame_width  # update to actual frame width for accurate angle estimation

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

    frame_count = 0
    frame_queue = deque()
    logging.info("ðŸš€ Processing started...")
    print("ðŸš€ Processing started...")

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame = cv2.resize(frame, (frame_width, frame_height), interpolation=cv2.INTER_AREA)
        frame_queue.append(frame)

        if len(frame_queue) == batch_size:
            process_batch(list(frame_queue), out)
            frame_queue.clear()

        frame_count += 1
        if frame_count % 50 == 0:
            logging.info(f"ðŸ“ˆ Processed {frame_count} frames...")
            print(f"ðŸ“ˆ Processed {frame_count} frames...")

    if frame_queue:
        process_batch(list(frame_queue), out)

    cap.release()
    out.release()
    print(f"âœ… Completed! Processed {frame_count} frames, saved to {output_path}")
    logging.info("ðŸ“¥ Downloading processed video...")
    print("ðŸ“¥ Downloading processed video...")

    files.download(output_path)

def process_batch(frames, out):
    # Preprocess each frame and build a batch tensor
    batch = []
    original_frames = []
    for frame in frames:
        original_frames.append(frame.copy())
        img = transform(frame).to(device)
        batch.append(img)
    batch_tensor = torch.stack(batch)

    with torch.no_grad():
        outputs = model(batch_tensor)

    for i, pred in enumerate(outputs):
        boxes = pred['boxes'].cpu()
        labels = pred['labels'].cpu()
        scores = pred['scores'].cpu()
        annotated = draw_boxes(original_frames[i], boxes, labels, scores)
        out.write(annotated)

# Set input and output paths for Colab
input_video = "/content/input_video.mp4"  # Ensure this video is uploaded
output_video = "/content/output_video.mp4"

# Run video processing
process_video(input_video, output_video, batch_size=8)